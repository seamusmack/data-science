{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372486cd-45e9-492f-a501-69150036440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== COOLING LOAD PREDICTION SYSTEM =====\n",
      "Existing data files not found. Generating synthetic database...\n",
      "\n",
      "Generated Dataset Statistics:\n",
      "Total samples: 1000\n",
      "Cooling load range: 0.00 to 100.00\n",
      "Binary outcome distribution: {1: 500, 0: 500}\n",
      "\n",
      "Dataset files created:\n",
      "- train.csv: 700 samples\n",
      "- validation.csv: 150 samples\n",
      "- test.csv: 150 samples (without outcome)\n",
      "- test_ground_truth.csv: Test set ground truth values\n",
      "- submissions_template.csv: Template for submissions\n",
      "\n",
      "===== DATA EXPLORATION =====\n",
      "Training data shape: (700, 26)\n",
      "\n",
      "Feature list:\n",
      "- roof_area\n",
      "- overall_height\n",
      "- floor_count\n",
      "- floor_area\n",
      "- glazing_area\n",
      "- glazing_area_distribution\n",
      "- wall_area\n",
      "- wall_u_value\n",
      "- roof_u_value\n",
      "- glazing_u_value\n",
      "- orientation\n",
      "- relative_compactness\n",
      "- heating_degree_days\n",
      "- cooling_degree_days\n",
      "- avg_summer_temp\n",
      "- insulation_quality\n",
      "- thermal_mass\n",
      "- shading_coefficient\n",
      "- ventilation_rate\n",
      "- occupancy_density\n",
      "- volume\n",
      "- surface_area\n",
      "- window_to_wall_ratio\n",
      "\n",
      "First few rows of training data:\n",
      "      roof_area  overall_height  floor_count  floor_area  glazing_area  \\\n",
      "288  150.289438        2.915039            2  458.745092     34.183660   \n",
      "9    297.825402        8.617812            2  101.771036     13.967630   \n",
      "377   58.145178        8.935217            3  158.843410     68.740263   \n",
      "602  111.953669        5.333205            3  157.813549     73.647689   \n",
      "429   90.973456       13.348979            3  326.513333     65.825983   \n",
      "\n",
      "     glazing_area_distribution   wall_area  wall_u_value  roof_u_value  \\\n",
      "288                   0.096695  583.963570      2.378799      0.204315   \n",
      "9                     0.430347  241.535475      0.639345      0.155596   \n",
      "377                   0.342069  517.252925      1.096546      0.417572   \n",
      "602                   0.125251  295.693568      1.513537      1.305725   \n",
      "429                   0.715985  403.586176      1.714593      0.282454   \n",
      "\n",
      "     glazing_u_value  ...  thermal_mass  shading_coefficient  \\\n",
      "288         2.780138  ...      0.341548             0.878216   \n",
      "9           3.639728  ...      0.186132             0.116463   \n",
      "377         2.806295  ...      0.009666             0.432615   \n",
      "602         1.637630  ...      0.542170             0.632710   \n",
      "429         4.860469  ...      0.368134             0.795451   \n",
      "\n",
      "     ventilation_rate  occupancy_density       volume  surface_area  \\\n",
      "288          0.590877           0.099019  1337.259770    884.542446   \n",
      "9            1.323159           0.046970   877.043657    837.186280   \n",
      "377          0.614606           0.099166  1419.300329    633.543280   \n",
      "602          0.465343           0.063736   841.652014    519.600907   \n",
      "429          1.324435           0.065516  4358.619518    585.533088   \n",
      "\n",
      "     window_to_wall_ratio  cooling_load_raw  outcome   id  \n",
      "288              0.058537         87.869956        1  288  \n",
      "9                0.057828          7.663709        0    9  \n",
      "377              0.132895         34.299034        1  377  \n",
      "602              0.249068         26.034704        0  602  \n",
      "429              0.163103         45.124938        1  429  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "No missing values found in the dataset.\n",
      "\n",
      "Basic statistics for numerical features:\n",
      "                           count         mean          std         min  \\\n",
      "roof_area                  700.0   220.686647   104.248088   51.771554   \n",
      "overall_height             700.0     8.850690     3.650537    2.540228   \n",
      "floor_count                700.0     2.137143     1.004165    1.000000   \n",
      "floor_area                 700.0   287.047288   121.051534   80.274424   \n",
      "glazing_area               700.0    44.952151    20.120488   10.002150   \n",
      "glazing_area_distribution  700.0     0.501938     0.288883    0.006184   \n",
      "wall_area                  700.0   354.719297   138.947184  120.685605   \n",
      "wall_u_value               700.0     1.280426     0.672598    0.150566   \n",
      "roof_u_value               700.0     0.786356     0.394933    0.100340   \n",
      "glazing_u_value            700.0     3.149090     1.368107    0.800259   \n",
      "orientation                700.0     1.542857     1.109018    0.000000   \n",
      "relative_compactness       700.0     0.754803     0.140283    0.501694   \n",
      "heating_degree_days        700.0  2742.035700  1330.160769  500.709851   \n",
      "cooling_degree_days        700.0  1610.249284   774.464664  303.249758   \n",
      "avg_summer_temp            700.0    27.486728     4.492705   20.021273   \n",
      "insulation_quality         700.0     0.499659     0.285812    0.000554   \n",
      "thermal_mass               700.0     0.492069     0.296845    0.000310   \n",
      "shading_coefficient        700.0     0.517938     0.281405    0.001146   \n",
      "ventilation_rate           700.0     0.919834     0.336192    0.310913   \n",
      "occupancy_density          700.0     0.056156     0.026316    0.010055   \n",
      "volume                     700.0  2534.930456  1549.550811  272.637408   \n",
      "surface_area               700.0   796.092590   253.134659  254.716119   \n",
      "window_to_wall_ratio       700.0     0.151744     0.101400    0.020498   \n",
      "cooling_load_raw           700.0    35.624643    20.418390    0.000000   \n",
      "outcome                    700.0     0.500000     0.500358    0.000000   \n",
      "id                         700.0   497.044286   287.707583    0.000000   \n",
      "\n",
      "                                   25%          50%          75%          max  \n",
      "roof_area                   124.482765   222.417560   312.543414   398.905988  \n",
      "overall_height                5.513428     8.988385    11.938738    14.974176  \n",
      "floor_count                   1.000000     2.000000     3.000000     5.000000  \n",
      "floor_area                  181.078096   286.562533   390.024980   499.814235  \n",
      "glazing_area                 27.383389    45.582817    62.328464    79.842457  \n",
      "glazing_area_distribution     0.257869     0.499476     0.739075     0.997672  \n",
      "wall_area                   235.306398   345.721277   477.917442   599.741127  \n",
      "wall_u_value                  0.698992     1.277518     1.821694     2.497426  \n",
      "roof_u_value                  0.452406     0.780935     1.116031     1.494418  \n",
      "glazing_u_value               2.008262     3.096346     4.313971     5.697575  \n",
      "orientation                   1.000000     2.000000     3.000000     3.000000  \n",
      "relative_compactness          0.640694     0.755146     0.875448     0.999837  \n",
      "heating_degree_days        1542.966395  2715.316809  3943.437161  4994.143168  \n",
      "cooling_degree_days         931.670194  1571.714767  2251.491201  2995.134274  \n",
      "avg_summer_temp              23.523614    27.236457    31.600981    34.974547  \n",
      "insulation_quality            0.248279     0.509715     0.754038     0.999805  \n",
      "thermal_mass                  0.243649     0.477804     0.751102     0.998175  \n",
      "shading_coefficient           0.285522     0.531727     0.759868     0.999150  \n",
      "ventilation_rate              0.645314     0.931321     1.196409     1.493940  \n",
      "occupancy_density             0.033952     0.055646     0.079233     0.099793  \n",
      "volume                     1306.527603  2218.808136  3545.072404  7084.518958  \n",
      "surface_area                599.549899   784.623349   974.664813  1381.858654  \n",
      "window_to_wall_ratio          0.076830     0.128396     0.199302     0.582515  \n",
      "cooling_load_raw             19.694167    31.301339    48.935432   100.000000  \n",
      "outcome                       0.000000     0.500000     1.000000     1.000000  \n",
      "id                          247.750000   496.000000   745.250000   998.000000  \n",
      "\n",
      "Target (outcome) distribution:\n",
      "outcome\n",
      "1    350\n",
      "0    350\n",
      "Name: count, dtype: int64\n",
      "outcome\n",
      "1    50.00%\n",
      "0    50.00%\n",
      "Name: proportion, dtype: object\n",
      "\n",
      "===== CREATING VISUALIZATIONS =====\n",
      "\n",
      "===== DATA PREPARATION =====\n",
      "Using separate validation set with 150 samples\n",
      "Prepared 23 features for modeling\n",
      "\n",
      "===== MODEL SELECTION & TRAINING =====\n",
      "Random Forest:\n",
      "  Cross-validation accuracy: 0.8971 (±0.0273)\n",
      "  Validation accuracy: 0.9533\n",
      "Gradient Boosting:\n",
      "  Cross-validation accuracy: 0.9143 (±0.0150)\n",
      "  Validation accuracy: 0.9600\n",
      "\n",
      "Best model: Gradient Boosting with validation accuracy 0.9600\n",
      "\n",
      "Detailed Evaluation of Best Model:\n",
      "Accuracy: 0.9600\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96        75\n",
      "           1       0.99      0.93      0.96        75\n",
      "\n",
      "    accuracy                           0.96       150\n",
      "   macro avg       0.96      0.96      0.96       150\n",
      "weighted avg       0.96      0.96      0.96       150\n",
      "\n",
      "\n",
      "Top 10 Important Features:\n",
      "  floor_area: 0.4612\n",
      "  occupancy_density: 0.3579\n",
      "  wall_u_value: 0.0285\n",
      "  wall_area: 0.0215\n",
      "  volume: 0.0171\n",
      "  surface_area: 0.0157\n",
      "  cooling_degree_days: 0.0115\n",
      "  shading_coefficient: 0.0115\n",
      "  ventilation_rate: 0.0102\n",
      "  floor_count: 0.0084\n",
      "\n",
      "===== FINAL PREDICTION =====\n",
      "Predictions saved to submissions.csv\n",
      "\n",
      "Test Set Accuracy: 0.9467\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95        75\n",
      "           1       0.97      0.92      0.95        75\n",
      "\n",
      "    accuracy                           0.95       150\n",
      "   macro avg       0.95      0.95      0.95       150\n",
      "weighted avg       0.95      0.95      0.95       150\n",
      "\n",
      "\n",
      "===== SAVING MODEL AND METADATA =====\n",
      "Model information saved to model_info.txt\n",
      "\n",
      "===== ANALYSIS COMPLETE =====\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function to generate a comprehensive synthetic dataset\n",
    "def generate_synthetic_database(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive synthetic housing dataset with cooling load outcome\n",
    "    This creates realistic house features that influence cooling load\n",
    "    \"\"\"\n",
    "    # Generate building features with realistic distributions\n",
    "    data = {\n",
    "        # Core building dimensions\n",
    "        'roof_area': np.random.uniform(50, 400, n_samples),  # square meters\n",
    "        'overall_height': np.random.uniform(2.5, 15, n_samples),  # meters\n",
    "        'floor_count': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.3, 0.4, 0.2, 0.07, 0.03]),\n",
    "        'floor_area': np.random.uniform(80, 500, n_samples),  # square meters\n",
    "        \n",
    "        # Building envelope\n",
    "        'glazing_area': np.random.uniform(10, 80, n_samples),  # square meters\n",
    "        'glazing_area_distribution': np.random.uniform(0, 1, n_samples),  # 0: equal, 1: mostly south\n",
    "        'wall_area': np.random.uniform(120, 600, n_samples),  # square meters\n",
    "        'wall_u_value': np.random.uniform(0.15, 2.5, n_samples),  # W/m²K - thermal transmittance\n",
    "        'roof_u_value': np.random.uniform(0.1, 1.5, n_samples),  # W/m²K - thermal transmittance\n",
    "        'glazing_u_value': np.random.uniform(0.8, 5.7, n_samples),  # W/m²K - thermal transmittance\n",
    "        \n",
    "        # Orientation and location\n",
    "        'orientation': np.random.choice([0, 1, 2, 3], n_samples),  # 0:N, 1:E, 2:S, 3:W\n",
    "        'relative_compactness': np.random.uniform(0.5, 1, n_samples),  # ratio of volume to surface area\n",
    "        \n",
    "        # Climate parameters\n",
    "        'heating_degree_days': np.random.uniform(500, 5000, n_samples),\n",
    "        'cooling_degree_days': np.random.uniform(300, 3000, n_samples),\n",
    "        'avg_summer_temp': np.random.uniform(20, 35, n_samples),  # celsius\n",
    "        \n",
    "        # Other building features\n",
    "        'insulation_quality': np.random.uniform(0, 1, n_samples),  # 0: poor, 1: excellent\n",
    "        'thermal_mass': np.random.uniform(0, 1, n_samples),  # 0: lightweight, 1: heavyweight\n",
    "        'shading_coefficient': np.random.uniform(0, 1, n_samples),  # 0: no shading, 1: full shading\n",
    "        'ventilation_rate': np.random.uniform(0.3, 1.5, n_samples),  # air changes per hour\n",
    "        'occupancy_density': np.random.uniform(0.01, 0.1, n_samples)  # people per square meter\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add derived features\n",
    "    df['volume'] = df['floor_area'] * df['overall_height']\n",
    "    df['surface_area'] = 2 * df['roof_area'] + df['wall_area']\n",
    "    df['window_to_wall_ratio'] = df['glazing_area'] / df['wall_area']\n",
    "    \n",
    "    # Calculate cooling load based on features (more complex model)\n",
    "    cooling_load = (\n",
    "        # Base load proportional to volume\n",
    "        0.05 * df['volume'] +\n",
    "        \n",
    "        # Gains through glazing (windows) affected by orientation and shading\n",
    "        0.8 * df['glazing_area'] * (5 - df['glazing_u_value']) * (1 - df['shading_coefficient']) * \n",
    "            (1 + 0.3 * (df['orientation'] == 2).astype(int)) +  # South-facing penalty\n",
    "        \n",
    "        # Conduction through walls\n",
    "        0.2 * df['wall_area'] * df['wall_u_value'] +\n",
    "        \n",
    "        # Conduction through roof\n",
    "        0.3 * df['roof_area'] * df['roof_u_value'] *\n",
    "            (1 + 0.2 * (1 - df['insulation_quality'])) +\n",
    "        \n",
    "        # Climate impact\n",
    "        0.001 * df['cooling_degree_days'] * (1 - df['relative_compactness']) +\n",
    "        0.03 * df['avg_summer_temp']**2 +\n",
    "        \n",
    "        # Internal gains\n",
    "        30 * df['occupancy_density'] * df['floor_area'] +\n",
    "        \n",
    "        # Cooling relief from ventilation\n",
    "        -15 * df['ventilation_rate'] * df['volume'] * 0.001 +\n",
    "        \n",
    "        # Add some noise for realism\n",
    "        np.random.normal(0, 20, n_samples)\n",
    "    )\n",
    "    \n",
    "    # Normalize cooling load to a reasonable range\n",
    "    cooling_load = (cooling_load - cooling_load.min()) / (cooling_load.max() - cooling_load.min()) * 100\n",
    "    \n",
    "    # Save raw cooling load for potential regression tasks\n",
    "    df['cooling_load_raw'] = cooling_load\n",
    "    \n",
    "    # Convert to binary outcome (0: low, 1: high) based on median\n",
    "    median_load = np.median(cooling_load)\n",
    "    df['outcome'] = (cooling_load > median_load).astype(int)\n",
    "    \n",
    "    # Add ID column\n",
    "    df['id'] = range(n_samples)\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    print(\"\\nGenerated Dataset Statistics:\")\n",
    "    print(f\"Total samples: {n_samples}\")\n",
    "    print(f\"Cooling load range: {cooling_load.min():.2f} to {cooling_load.max():.2f}\")\n",
    "    print(f\"Binary outcome distribution: {df['outcome'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Split into train, validation and test\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['outcome'])\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['outcome'])\n",
    "    \n",
    "    # Save files\n",
    "    train_df.to_csv('train.csv', index=False)\n",
    "    val_df.to_csv('validation.csv', index=False)\n",
    "    \n",
    "    # Create test set without outcome\n",
    "    test_features = test_df.drop(['outcome', 'cooling_load_raw'], axis=1)\n",
    "    test_features.to_csv('test.csv', index=False)\n",
    "    \n",
    "    # Save ground truth for later evaluation\n",
    "    test_df[['id', 'outcome', 'cooling_load_raw']].to_csv('test_ground_truth.csv', index=False)\n",
    "    \n",
    "    # Create empty submissions file template\n",
    "    submissions = pd.DataFrame({'id': test_df['id'], 'outcome': [0] * len(test_df)})\n",
    "    submissions.to_csv('submissions_template.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nDataset files created:\")\n",
    "    print(f\"- train.csv: {len(train_df)} samples\")\n",
    "    print(f\"- validation.csv: {len(val_df)} samples\")\n",
    "    print(f\"- test.csv: {len(test_df)} samples (without outcome)\")\n",
    "    print(f\"- test_ground_truth.csv: Test set ground truth values\")\n",
    "    print(f\"- submissions_template.csv: Template for submissions\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Full prediction program with expanded capabilities\n",
    "def predict_cooling_load():\n",
    "    \"\"\"\n",
    "    Complete program to analyze cooling load data, build and evaluate models,\n",
    "    and generate predictions for submission.\n",
    "    \"\"\"\n",
    "    print(\"===== COOLING LOAD PREDICTION SYSTEM =====\")\n",
    "    \n",
    "    # Try to load existing data, generate synthetic if files don't exist\n",
    "    try:\n",
    "        train_data = pd.read_csv('train.csv')\n",
    "        test_data = pd.read_csv('test.csv')\n",
    "        try:\n",
    "            val_data = pd.read_csv('validation.csv')\n",
    "            print(\"Loaded existing data files (train, validation, and test)\")\n",
    "        except FileNotFoundError:\n",
    "            val_data = None\n",
    "            print(\"Loaded existing data files (train and test only)\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Existing data files not found. Generating synthetic database...\")\n",
    "        train_data, val_data, test_data_with_outcome = generate_synthetic_database(n_samples=1000)\n",
    "        test_data = pd.read_csv('test.csv')\n",
    "    \n",
    "    # ===== DATA EXPLORATION =====\n",
    "    print(\"\\n===== DATA EXPLORATION =====\")\n",
    "    print(f\"Training data shape: {train_data.shape}\")\n",
    "    print(\"\\nFeature list:\")\n",
    "    for col in train_data.columns:\n",
    "        if col not in ['id', 'outcome', 'cooling_load_raw']:\n",
    "            print(f\"- {col}\")\n",
    "    \n",
    "    print(\"\\nFirst few rows of training data:\")\n",
    "    print(train_data.head())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = train_data.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"\\nMissing values in training data:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\"\\nNo missing values found in the dataset.\")\n",
    "    \n",
    "    print(\"\\nBasic statistics for numerical features:\")\n",
    "    print(train_data.describe().T)\n",
    "    \n",
    "    print(\"\\nTarget (outcome) distribution:\")\n",
    "    outcome_counts = train_data['outcome'].value_counts()\n",
    "    print(outcome_counts)\n",
    "    print(train_data['outcome'].value_counts(normalize=True).map(lambda x: f\"{x:.2%}\"))\n",
    "    \n",
    "    # ===== VISUALIZATIONS =====\n",
    "    print(\"\\n===== CREATING VISUALIZATIONS =====\")\n",
    "    \n",
    "    # Target distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='outcome', data=train_data)\n",
    "    plt.title('Cooling Load Distribution', fontsize=14)\n",
    "    plt.xlabel('Outcome (0: Low, 1: High)', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.xticks([0, 1], ['Low Cooling Load', 'High Cooling Load'])\n",
    "    plt.savefig('cooling_load_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    features = train_data.drop(['id', 'outcome', 'cooling_load_raw'], axis=1, errors='ignore')\n",
    "    correlation = features.corr()\n",
    "    mask = np.triu(correlation)\n",
    "    sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f', mask=mask)\n",
    "    plt.title('Feature Correlation Matrix', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_correlation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot histograms for key features\n",
    "    key_features = features.columns[:min(10, len(features.columns))]\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(key_features):\n",
    "        plt.subplot(3, 4, i+1)\n",
    "        sns.histplot(train_data[feature], kde=True)\n",
    "        plt.title(feature)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_distributions.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot boxplots for features by outcome\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(key_features):\n",
    "        plt.subplot(3, 4, i+1)\n",
    "        sns.boxplot(x='outcome', y=feature, data=train_data)\n",
    "        plt.title(f'{feature} by Outcome')\n",
    "        plt.xlabel('Cooling Load')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('features_by_outcome.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # ===== DATA PREPARATION =====\n",
    "    print(\"\\n===== DATA PREPARATION =====\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    if 'outcome' in train_data.columns:\n",
    "        X_train = train_data.drop(['id', 'outcome', 'cooling_load_raw'], axis=1, errors='ignore')\n",
    "        y_train = train_data['outcome']\n",
    "    else:\n",
    "        raise ValueError(\"Training data does not contain 'outcome' column\")\n",
    "    \n",
    "    # Prepare validation data if available\n",
    "    if val_data is not None and 'outcome' in val_data.columns:\n",
    "        X_val = val_data.drop(['id', 'outcome', 'cooling_load_raw'], axis=1, errors='ignore')\n",
    "        y_val = val_data['outcome']\n",
    "        print(f\"Using separate validation set with {len(X_val)} samples\")\n",
    "    else:\n",
    "        # Create validation split if no validation data provided\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        print(f\"Created validation split: {len(X_train)} training samples, {len(X_val)} validation samples\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_data.drop(['id'], axis=1, errors='ignore')\n",
    "    test_ids = test_data['id'] if 'id' in test_data.columns else np.arange(len(test_data))\n",
    "    \n",
    "    # Check for feature consistency\n",
    "    train_features = set(X_train.columns)\n",
    "    test_features = set(X_test.columns)\n",
    "    \n",
    "    if train_features != test_features:\n",
    "        missing_in_test = train_features - test_features\n",
    "        extra_in_test = test_features - train_features\n",
    "        \n",
    "        if missing_in_test:\n",
    "            print(f\"Warning: Test data is missing features: {missing_in_test}\")\n",
    "        if extra_in_test:\n",
    "            print(f\"Warning: Test data has extra features: {extra_in_test}\")\n",
    "            X_test = X_test.drop(list(extra_in_test), axis=1, errors='ignore')\n",
    "        \n",
    "        # Ensure feature alignment\n",
    "        X_test = X_test[X_train.columns]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Prepared {X_train_scaled.shape[1]} features for modeling\")\n",
    "    \n",
    "    # ===== MODEL SELECTION & TRAINING =====\n",
    "    print(\"\\n===== MODEL SELECTION & TRAINING =====\")\n",
    "    \n",
    "    # Define models to evaluate\n",
    "    models = {\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            random_state=42\n",
    "        ),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Evaluate models with cross-validation\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    model_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Cross-validation on training data\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "        avg_cv_score = cv_scores.mean()\n",
    "        \n",
    "        # Fit model on full training data\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_predictions = model.predict(X_val_scaled)\n",
    "        val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "        \n",
    "        model_results[name] = {\n",
    "            'cv_score': avg_cv_score,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Cross-validation accuracy: {avg_cv_score:.4f} (±{cv_scores.std():.4f})\")\n",
    "        print(f\"  Validation accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Track best model\n",
    "        if val_accuracy > best_score:\n",
    "            best_score = val_accuracy\n",
    "            best_model = model\n",
    "            best_model_name = name\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name} with validation accuracy {best_score:.4f}\")\n",
    "    \n",
    "    # Evaluate best model in detail\n",
    "    val_predictions = best_model.predict(X_val_scaled)\n",
    "    \n",
    "    print(\"\\nDetailed Evaluation of Best Model:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_val, val_predictions):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_val, val_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14)\n",
    "    plt.xlabel('Predicted', fontsize=12)\n",
    "    plt.ylabel('Actual', fontsize=12)\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance for tree-based models\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "        plt.title(f'Top 15 Feature Importance - {best_model_name}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"\\nTop 10 Important Features:\")\n",
    "        for i, row in feature_importance.head(10).iterrows():\n",
    "            print(f\"  {row['Feature']}: {row['Importance']:.4f}\")\n",
    "    \n",
    "    # ===== FINAL PREDICTION =====\n",
    "    print(\"\\n===== FINAL PREDICTION =====\")\n",
    "    \n",
    "    # Train final model on combined training and validation data\n",
    "    final_model = best_model.__class__(**best_model.get_params())\n",
    "    \n",
    "    # Combine training and validation data\n",
    "    X_combined = np.vstack((X_train_scaled, X_val_scaled))\n",
    "    y_combined = np.concatenate((y_train, y_val))\n",
    "    \n",
    "    # Fit the model\n",
    "    final_model.fit(X_combined, y_combined)\n",
    "    \n",
    "    # Generate predictions\n",
    "    test_predictions = final_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Create submission file\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'outcome': test_predictions\n",
    "    })\n",
    "    \n",
    "    # Save predictions\n",
    "    submission.to_csv(\"submissions.csv\", index=False)\n",
    "    print(\"Predictions saved to submissions.csv\")\n",
    "    \n",
    "    # Check if we have ground truth for evaluation\n",
    "    try:\n",
    "        ground_truth = pd.read_csv('test_ground_truth.csv')\n",
    "        if 'outcome' in ground_truth.columns:\n",
    "            true_outcomes = ground_truth['outcome']\n",
    "            test_accuracy = accuracy_score(true_outcomes, test_predictions)\n",
    "            print(f\"\\nTest Set Accuracy: {test_accuracy:.4f}\")\n",
    "            \n",
    "            print(\"Classification Report on Test Set:\")\n",
    "            print(classification_report(true_outcomes, test_predictions))\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nNo ground truth file found for test set evaluation.\")\n",
    "    \n",
    "    # ===== SAVE MODEL =====\n",
    "    print(\"\\n===== SAVING MODEL AND METADATA =====\")\n",
    "    \n",
    "    # Create model info dictionary\n",
    "    model_info = {\n",
    "        'model_type': best_model_name,\n",
    "        'train_size': len(X_train),\n",
    "        'validation_size': len(X_val),\n",
    "        'test_size': len(X_test),\n",
    "        'features_used': X_train.columns.tolist(),\n",
    "        'validation_accuracy': best_score,\n",
    "        'feature_importance': feature_importance.to_dict() if hasattr(best_model, 'feature_importances_') else None\n",
    "    }\n",
    "    \n",
    "    # Save model metadata\n",
    "    with open('model_info.txt', 'w') as f:\n",
    "        f.write(\"===== COOLING LOAD PREDICTION MODEL =====\\n\\n\")\n",
    "        f.write(f\"Model Type: {model_info['model_type']}\\n\")\n",
    "        f.write(f\"Validation Accuracy: {model_info['validation_accuracy']:.4f}\\n\\n\")\n",
    "        f.write(f\"Training Size: {model_info['train_size']}\\n\")\n",
    "        f.write(f\"Validation Size: {model_info['validation_size']}\\n\")\n",
    "        f.write(f\"Test Size: {model_info['test_size']}\\n\\n\")\n",
    "        f.write(\"Top 10 Important Features:\\n\")\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            for i, row in feature_importance.head(10).iterrows():\n",
    "                f.write(f\"  {row['Feature']}: {row['Importance']:.4f}\\n\")\n",
    "    \n",
    "    print(\"Model information saved to model_info.txt\")\n",
    "    print(\"\\n===== ANALYSIS COMPLETE =====\")\n",
    "    \n",
    "    return final_model, submission\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    final_model, submission = predict_cooling_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938ca18-3465-44c2-842f-0b7a4fb14784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
